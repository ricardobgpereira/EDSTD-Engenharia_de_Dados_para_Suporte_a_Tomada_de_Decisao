{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4603bb7-d1d2-4718-87c5-170c6d50feba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdfs in /opt/conda/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from hdfs) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from hdfs) (2.28.1)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs) (0.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.7.0->hdfs) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d532eb-4adf-4ce5-bdc4-39a379f4a17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Qualidade_NYC/bronze/Air_Quality.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import PathLike\n",
    "from hdfs import InsecureClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from delta import *\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, FloatType\n",
    "\n",
    "client=InsecureClient(\"http://hdfs-nn:9870\",user='anonymous')\n",
    "\n",
    "from_path= \"./Air_Quality.csv\"\n",
    "to_path=\"/Qualidade_NYC/bronze/Air_Quality.csv\"\n",
    "\n",
    "client.delete(to_path)\n",
    "client.upload(to_path,from_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eeed1ed-1c19-4ad3-b7e4-58064148ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = 'hdfs://hdfs-nn:9000/warehouse'\n",
    "\n",
    "builder = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "\n",
    "spark = spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded94d4b-9e3f-41d5-9c52-d831da00a006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m hdfs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://hdfs-nn:9000/demo/bronze/Air_Quality.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#define the schema for the dataframe\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m customSchema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),        \n\u001b[1;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndicator_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasure\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasure_Info\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeo_Type_Name\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     12\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeo_Join_ID \u001b[39m\u001b[38;5;124m\"\u001b[39m,IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeo_Place_Name \u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime_Period \u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     15\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart_Period\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_Value\u001b[39m\u001b[38;5;124m\"\u001b[39m,FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(),\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     21\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m spark \\\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;241m.\u001b[39mread\\\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;241m.\u001b[39mschema(customSchema) \\\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;241m.\u001b[39mcsv(hdfs_path)\n\u001b[1;32m     27\u001b[0m sales_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "#read hdfs file to dataframe\n",
    "#\n",
    "hdfs_path = \"hdfs://hdfs-nn:9000/demo/bronze/Air_Quality.csv\"\n",
    "#define the schema for the dataframe\n",
    "customSchema = StructType([\n",
    "    StructField(\"Unique_ID\", StringType(), True),        \n",
    "    StructField(\"Indicator_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Measure\", StringType(), True),\n",
    "    StructField(\"Measure_Info\", StringType(), True),\n",
    "    StructField(\"Geo_Type_Name\",StringType(), True),\n",
    "    StructField(\"Geo_Join_ID \",IntegerType(), True),\n",
    "    StructField(\"Geo_Place_Name \",StringType(), True),\n",
    "    StructField(\"Time_Period \",StringType(), True),\n",
    "    StructField(\"Start_Period\",StringType(), True),\n",
    "    StructField(\"Data_Value\",FloatType(), True),\n",
    "    StructField(\"Message\",StringType(),True)\n",
    "    \n",
    "])\n",
    "\n",
    "sales_df = spark \\\n",
    "            .read\\\n",
    "            .option(\"delimiter\",\";\")\\\n",
    "            .option(\"header\",\"True\")\\\n",
    "            .schema(customSchema) \\\n",
    "            .csv(hdfs_path)\n",
    "sales_df.show()\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a1defa-09b5-472f-b1c1-163d3ab763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://hdfs-nn:9000/demo/bronze/air_quality.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253ff6d-365b-430d-9771-42fd059bc96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
